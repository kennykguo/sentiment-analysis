{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":8722711,"sourceType":"datasetVersion","datasetId":5234400},{"sourceId":8723520,"sourceType":"datasetVersion","datasetId":5229156},{"sourceId":184224327,"sourceType":"kernelVersion"}],"dockerImageVersionId":30733,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# https://www.kaggle.com/code/emirkocak/in-depth-series-sentiment-analysis-w-transformers\nimport torch\nimport torch.nn as nn\nimport torchtext\nfrom torchtext.data.functional import generate_sp_model, load_sp_model, sentencepiece_tokenizer, sentencepiece_numericalizer\nfrom torchtext.vocab import build_vocab_from_iterator\nimport torchtext.transforms as T\nfrom torch.nn import functional as F\nimport torch.optim as optim\n\nimport sys\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport io\nimport math\nfrom torch.utils.data import Dataset, DataLoader\nimport torch.utils.data.dataloader as dataloader\nimport os\nimport re","metadata":{"execution":{"iopub.status.busy":"2024-07-02T01:14:16.572375Z","iopub.execute_input":"2024-07-02T01:14:16.572761Z","iopub.status.idle":"2024-07-02T01:14:22.372807Z","shell.execute_reply.started":"2024-07-02T01:14:16.572731Z","shell.execute_reply":"2024-07-02T01:14:22.372010Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"root = \"/kaggle/input/movies\"","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-07-02T01:14:22.374540Z","iopub.execute_input":"2024-07-02T01:14:22.375158Z","iopub.status.idle":"2024-07-02T01:14:22.379361Z","shell.execute_reply.started":"2024-07-02T01:14:22.375120Z","shell.execute_reply":"2024-07-02T01:14:22.378451Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"# # Specify the correct encoding (e.g., 'utf-8', 'latin-1', etc.)\n# with open(input_file, encoding='utf-8') as f:  # Adjust 'utf-8' to the actual encoding if different\n#     with open(output_file, \"w\", encoding='utf-8') as f2:\n#         for i, line in enumerate(f):\n#             try:\n#                 text_only = \"\".join(line.split(\",\")[:-1])\n#                 f2.write(text_only + \"\\n\")\n#             except IndexError:\n#                 print(f\"Error processing line {i}: {line.strip()}\")\n\nwith open(os.path.join(root, \"reviews.csv\"), encoding='latin-1') as f:\n        with open(os.path.join(\"/kaggle/working\", \"data.txt\"), \"w\") as f2:\n            for line in f:\n                text_only = \"\".join(line.split(\",\")[:-1])\n                filtered = re.sub(r'\\\\|\\\\n|;', ' ', text_only.replace('\"', ' ').replace('\\n', ' ')) # Replaces double quotes with a space, and \\n with a space\n                # Replaces \\\\, \\\\n, and; with a space\n                # Replaces HTML codes with real characters\n                filtered = filtered.replace(' #39;', \"'\")\n                filtered = filtered.replace(' #38;', \"&\")\n                filtered = filtered.replace(' #36;', \"$\")\n                filtered = filtered.replace(' #151;', \"-\")\n                f2.write(filtered.lower() + \"\\n\")","metadata":{"execution":{"iopub.status.busy":"2024-07-02T01:14:22.381172Z","iopub.execute_input":"2024-07-02T01:14:22.381529Z","iopub.status.idle":"2024-07-02T01:14:24.475044Z","shell.execute_reply.started":"2024-07-02T01:14:22.381497Z","shell.execute_reply":"2024-07-02T01:14:24.474084Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"# with open(os.path.join(root, \"reviews.csv\")) as f:\n#         with open(os.path.join(\"/kaggle/working\", \"data.txt\"), \"w\") as f2:\n#             for line in f:\n#                 text_only = \"\".join(line.split(\",\")[:-1])\n#                 filtered = re.sub(r'\\\\|\\\\n|;', ' ', text_only.replace('\"', ' ').replace('\\n', ' ')) # Replaces double quotes with a space, and \\n with a space\n#                 # Replaces \\\\, \\\\n, and; with a space\n#                 # Replaces HTML codes with real characters\n#                 filtered = filtered.replace(' #39;', \"'\")\n#                 filtered = filtered.replace(' #38;', \"&\")\n#                 filtered = filtered.replace(' #36;', \"$\")\n#                 filtered = filtered.replace(' #151;', \"-\")\n#                 f2.write(filtered.lower() + \"\\n\")","metadata":{"execution":{"iopub.status.busy":"2024-07-02T01:14:24.476806Z","iopub.execute_input":"2024-07-02T01:14:24.477098Z","iopub.status.idle":"2024-07-02T01:14:24.481776Z","shell.execute_reply.started":"2024-07-02T01:14:24.477072Z","shell.execute_reply":"2024-07-02T01:14:24.480610Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"# Generate the SentencePiece tokenizer\n# Text tokenizer and detokenizer\n# It will tokenize words into subpieces instead of words\n# This function will create a set of subtokens to fit the set vocabulary size\n# There will always be enough subwords to subtokenize a dataset if you think about it :) -> max 2 length pairs = 26!\n# Saved in the home directory\ngenerate_sp_model(os.path.join(\"/kaggle/working\", \"data.txt\"), vocab_size=20000, model_prefix='/kaggle/working/transformer')","metadata":{"execution":{"iopub.status.busy":"2024-07-02T01:16:12.631292Z","iopub.execute_input":"2024-07-02T01:16:12.631678Z","iopub.status.idle":"2024-07-02T01:17:10.901675Z","shell.execute_reply.started":"2024-07-02T01:16:12.631650Z","shell.execute_reply":"2024-07-02T01:17:10.900687Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stderr","text":"sentencepiece_trainer.cc(177) LOG(INFO) Running command: --input=/kaggle/working/data.txt --model_prefix=/kaggle/working/transformer --vocab_size=20000 --model_type=unigram\nsentencepiece_trainer.cc(77) LOG(INFO) Starts training with : \ntrainer_spec {\n  input: /kaggle/working/data.txt\n  input_format: \n  model_prefix: /kaggle/working/transformer\n  model_type: UNIGRAM\n  vocab_size: 20000\n  self_test_sample_size: 0\n  character_coverage: 0.9995\n  input_sentence_size: 0\n  shuffle_input_sentence: 1\n  seed_sentencepiece_size: 1000000\n  shrinking_factor: 0.75\n  max_sentence_length: 4192\n  num_threads: 16\n  num_sub_iterations: 2\n  max_sentencepiece_length: 16\n  split_by_unicode_script: 1\n  split_by_number: 1\n  split_by_whitespace: 1\n  split_digits: 0\n  treat_whitespace_as_suffix: 0\n  required_chars: \n  byte_fallback: 0\n  vocabulary_output_piece_score: 1\n  train_extremely_large_corpus: 0\n  hard_vocab_limit: 1\n  use_all_vocab: 0\n  unk_id: 0\n  bos_id: 1\n  eos_id: 2\n  pad_id: -1\n  unk_piece: <unk>\n  bos_piece: <s>\n  eos_piece: </s>\n  pad_piece: <pad>\n  unk_surface:  ⁇ \n}\nnormalizer_spec {\n  name: nmt_nfkc\n  add_dummy_prefix: 1\n  remove_extra_whitespaces: 1\n  escape_whitespaces: 1\n  normalization_rule_tsv: \n}\ndenormalizer_spec {}\ntrainer_interface.cc(319) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\ntrainer_interface.cc(174) LOG(INFO) Loading corpus: /kaggle/working/data.txt\ntrainer_interface.cc(346) LOG(WARNING) Found too long line (4998 > 4192).\ntrainer_interface.cc(348) LOG(WARNING) Too long lines are skipped in the training.\ntrainer_interface.cc(349) LOG(WARNING) The maximum length can be changed with --max_sentence_length=<size> flag.\ntrainer_interface.cc(375) LOG(INFO) Loaded all 48838 sentences\ntrainer_interface.cc(381) LOG(INFO) Skipped 1162 too long sentences.\ntrainer_interface.cc(390) LOG(INFO) Adding meta_piece: <unk>\ntrainer_interface.cc(390) LOG(INFO) Adding meta_piece: <s>\ntrainer_interface.cc(390) LOG(INFO) Adding meta_piece: </s>\ntrainer_interface.cc(395) LOG(INFO) Normalizing sentences...\ntrainer_interface.cc(456) LOG(INFO) all chars count=57894810\ntrainer_interface.cc(467) LOG(INFO) Done: 99.9505% characters are covered.\ntrainer_interface.cc(477) LOG(INFO) Alphabet size=47\ntrainer_interface.cc(478) LOG(INFO) Final character coverage=0.999505\ntrainer_interface.cc(510) LOG(INFO) Done! preprocessed 48838 sentences.\nunigram_model_trainer.cc(138) LOG(INFO) Making suffix array...\nunigram_model_trainer.cc(142) LOG(INFO) Extracting frequent sub strings...\nunigram_model_trainer.cc(193) LOG(INFO) Initialized 258153 seed sentencepieces\ntrainer_interface.cc(516) LOG(INFO) Tokenizing input sentences with whitespace: 48838\ntrainer_interface.cc(526) LOG(INFO) Done! 268970\nunigram_model_trainer.cc(488) LOG(INFO) Using 268970 sentences for EM training\nunigram_model_trainer.cc(504) LOG(INFO) EM sub_iter=0 size=94535 obj=10.2153 num_tokens=643572 num_tokens/piece=6.80776\nunigram_model_trainer.cc(504) LOG(INFO) EM sub_iter=1 size=79016 obj=8.0744 num_tokens=648526 num_tokens/piece=8.20753\nunigram_model_trainer.cc(504) LOG(INFO) EM sub_iter=0 size=59259 obj=8.03185 num_tokens=670770 num_tokens/piece=11.3193\nunigram_model_trainer.cc(504) LOG(INFO) EM sub_iter=1 size=59241 obj=8.02515 num_tokens=671339 num_tokens/piece=11.3323\nunigram_model_trainer.cc(504) LOG(INFO) EM sub_iter=0 size=44429 obj=8.05182 num_tokens=707101 num_tokens/piece=15.9153\nunigram_model_trainer.cc(504) LOG(INFO) EM sub_iter=1 size=44428 obj=8.04838 num_tokens=707030 num_tokens/piece=15.9141\nunigram_model_trainer.cc(504) LOG(INFO) EM sub_iter=0 size=33321 obj=8.09372 num_tokens=748324 num_tokens/piece=22.458\nunigram_model_trainer.cc(504) LOG(INFO) EM sub_iter=1 size=33320 obj=8.08432 num_tokens=748295 num_tokens/piece=22.4578\nunigram_model_trainer.cc(504) LOG(INFO) EM sub_iter=0 size=24990 obj=8.15818 num_tokens=793054 num_tokens/piece=31.7349\nunigram_model_trainer.cc(504) LOG(INFO) EM sub_iter=1 size=24990 obj=8.14388 num_tokens=792971 num_tokens/piece=31.7315\nunigram_model_trainer.cc(504) LOG(INFO) EM sub_iter=0 size=22000 obj=8.18201 num_tokens=811808 num_tokens/piece=36.9004\nunigram_model_trainer.cc(504) LOG(INFO) EM sub_iter=1 size=22000 obj=8.17505 num_tokens=811783 num_tokens/piece=36.8992\ntrainer_interface.cc(604) LOG(INFO) Saving model: /kaggle/working/transformer.model\ntrainer_interface.cc(615) LOG(INFO) Saving vocabs: /kaggle/working/transformer.vocab\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class IMDB(Dataset):\n    def __init__(self, root):\n        self.root = root\n\n        # Reads the file into a pandas DataFrame with Latin-1 encoding\n        self.df = pd.read_csv(os.path.join(root, \"reviews.csv\"), names=[\"Article\", \"Class\"], encoding='latin-1')\n\n        # Replaces empty entries with a space\n        self.df.fillna('', inplace=True)\n\n        # Clean the 'Article' column\n        self.df['Article'] = self.df['Article'].str.replace(r'\\\\n|\\\\|\\\\r|\\\\r\\\\n|\\n|\"', ' ', regex=True)\n        self.df['Article'] = self.df['Article'].replace({' #39;': \"'\", ' #38;': \"&\", ' #36;': \"$\", ' #151;': \"-\"}, regex=True)\n\n        # Shuffle the DataFrame\n        self.df = self.df.sample(frac=1).reset_index(drop=True)\n\n    # To use for DataLoader\n    def __getitem__(self, index):\n        text = self.df.loc[index][\"Article\"].lower()\n        \n        class_label = self.df.loc[index][\"Class\"]\n\n        if class_label == 'positive':\n            class_index = 1\n        else:\n            class_index = 0\n            \n        return class_label, text\n\n    def __len__(self):\n        return len(self.df)\n\n# Example usage:\ntrain_dataset = IMDB(root)\nprint(len(train_dataset))\nprint(train_dataset.df.loc[0][\"Article\"])\nprint(train_dataset.df.loc[0][\"Class\"])\n","metadata":{"execution":{"iopub.status.busy":"2024-07-02T01:19:31.169085Z","iopub.execute_input":"2024-07-02T01:19:31.169847Z","iopub.status.idle":"2024-07-02T01:19:33.773360Z","shell.execute_reply.started":"2024-07-02T01:19:31.169813Z","shell.execute_reply":"2024-07-02T01:19:33.772445Z"},"trusted":true},"execution_count":22,"outputs":[{"name":"stdout","text":"50000\nI rented this by mistake. I thought, after a cursory examination of the box, that this was a time-travel/sci-fi story. Instead, it's a  Christian  story, and I suppose is fairly typical example. If you are sold on the message you probably will overlook the awkwardness of the plot/acting/etc., but I found it rather painful.  I have to admit that I'm bothered by the rewriting of history in this story. It paints the 1890's as some sort of paradise of family values and morality (a character is aghast that 5% of marriages end in divorce!), but it overlooks very unsavory sides of this  highly moral  society (rigid racial, sexual, and social discrimination were widespread, for instance). And at one point the hero complains to a clothing store owner about things that sound not all that different than the complaints of some Iranian leaders about women's clothing styles (as reported in a recent WSJ). Overall, thought, I suppose that it's the sort of thing you'll like if you like this sort of thing, and it's certainly wholesome...\n0\n","output_type":"stream"}]},{"cell_type":"code","source":"# Split 90% - 10%\nvalidation_split = 0.9\n\n# Total train examples\nn_train_examples = int(len(train_dataset) * validation_split)\n\n# Total validation examples\nn_valid_examples = len(train_dataset) - n_train_examples\n\n# Splits them based on values provided\ntrain_data, valid_data = torch.utils.data.random_split(train_dataset, [n_train_examples, n_valid_examples], generator=torch.Generator().manual_seed(42))","metadata":{"execution":{"iopub.status.busy":"2024-07-02T01:19:33.775123Z","iopub.execute_input":"2024-07-02T01:19:33.775427Z","iopub.status.idle":"2024-07-02T01:19:33.784577Z","shell.execute_reply.started":"2024-07-02T01:19:33.775389Z","shell.execute_reply":"2024-07-02T01:19:33.783689Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"# Create dataloaders for the training and testing datasets\n# Dataloaders allow for batching, shuffling\n\nbatch_size = 128\n\ntrain_loader = dataloader.DataLoader(train_data, shuffle=True, batch_size=batch_size, drop_last = True)\n\ntest_loader = dataloader.DataLoader(valid_data, shuffle=True, batch_size=batch_size, drop_last = True)","metadata":{"execution":{"iopub.status.busy":"2024-07-02T01:19:33.785777Z","iopub.execute_input":"2024-07-02T01:19:33.786225Z","iopub.status.idle":"2024-07-02T01:19:33.805024Z","shell.execute_reply.started":"2024-07-02T01:19:33.786181Z","shell.execute_reply":"2024-07-02T01:19:33.804209Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"def yield_tokens(file_path):\n    with io.open(file_path, encoding='utf-8') as f:\n        # Iterate through each line in the file\n        for line in f:\n            # Accesses the vocab file, splits the line by tab, and gets the first entry (the actual token)\n            # Yield the token from the first column (split by tab)\n            yield [line.split(\"\\t\")[0]]\n\n# Build a vocabulary from the tokens yielded by the yield_tokens function\n    # <pad> is a padding token that is added to the end of a sentence to ensure the length of all sequences in a batch is the same\n    # <sos> signals the \"Start-Of-Sentence\" aka the start of the sequence\n    # <eos> signal the \"End-Of-Sentence\" aka the end of the sequence\n    # <unk> \"unknown\" token is used if a token is not contained in the vocab\n# From torchtext library (build_vocab_from_iterator)\n# Builds a generator object, that is treated like an iterator\nvocab = build_vocab_from_iterator(yield_tokens(\"/kaggle/working/transformer.vocab\"), specials=['<sos>', '<pad>', '<eos>', '<unk>'], special_first=True)\n\n# Set the default index for unknown tokens to the index of the '<unk>' token\nvocab.set_default_index(vocab['<unk>'])","metadata":{"execution":{"iopub.status.busy":"2024-07-02T01:19:35.078920Z","iopub.execute_input":"2024-07-02T01:19:35.079278Z","iopub.status.idle":"2024-07-02T01:19:35.248913Z","shell.execute_reply.started":"2024-07-02T01:19:35.079240Z","shell.execute_reply":"2024-07-02T01:19:35.247789Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"# Maximum sequence length for text inputs\nmax_len = 256\n\n# Data transform to turn text into vocab tokens\ntext_transform = T.Sequential(\n    # Tokenize with pre-existing Tokenizer\n    T.SentencePieceTokenizer(\"/kaggle/working/transformer.model\"),\n    # Converts the sentences to indices based on given vocabulary\n    T.VocabTransform(vocab=vocab),\n    # Add <sos> at the beginning of each sentence. 1 because the index for <sos> in the vocabulary is 1 as seen in previous section\n    T.AddToken(vocab['<sos>'], begin=True),\n    # Crop the sentence if it is longer than the max length minus 2 to accommodate <sos> and <eos> tokens\n    T.Truncate(max_seq_len=max_len-2),\n    # Add <eos> at the end of each sentence. 2 because the index for <eos> in the vocabulary is 2 as seen in previous section\n    T.AddToken(vocab['<eos>'], begin=False),\n    # Convert the list of lists to a tensor. This will also pad a sentence with the <pad> token if it is shorter than the max length.\n    # This ensures all sentences are the same length!\n    T.ToTensor(padding_value=vocab['<pad>']),\n    # Pad the sequence to ensure it's exactly max_len tokens long\n    T.PadTransform(max_length=max_len, pad_value=vocab['<pad>']),\n)","metadata":{"execution":{"iopub.status.busy":"2024-07-02T01:19:35.848341Z","iopub.execute_input":"2024-07-02T01:19:35.848759Z","iopub.status.idle":"2024-07-02T01:19:35.909099Z","shell.execute_reply.started":"2024-07-02T01:19:35.848726Z","shell.execute_reply":"2024-07-02T01:19:35.908070Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class TokenDrop(nn.Module):\n    \"\"\" For a batch of tokens indices, randomly replace a non-specical token with <pad>.\n    prob (float): probability of dropping a token\n    pad_token (int): index for the <pad> token\n    num_special (int): Number of special tokens, assumed to be at the start of the vocab\n    \"\"\"\n\n    def __init__(self, prob=0.1, pad_token=0, num_special=4):\n        self.prob = prob\n        self.num_special = num_special\n        self.pad_token = pad_token\n\n    def __call__(self, sample):\n        # Randomly sample a bernoulli distribution with p = prob\n        # Create a mask where 1 means we will replace that token\n        # Discrete probability distribution\n        # Here we want to treat the ones as the indexes to drop\n        mask = torch.bernoulli(self.prob * torch.ones_like(sample)).long()\n        \n        # Only replace if the token is not a special token\n        # Ones or zeros. If cannot drop, 0, if can drop, 1\n        can_drop = (sample >= self.num_special).long()\n        # Multiply together to get the corresponding tokens to be dropped and not dropped\n        # Here, 1 represents drop, 0 represents do not drop\n        mask = mask * can_drop\n\n        # Make a mask of pad_token to use for replacing dropped indices with the pad_token\n        replace_with = (self.pad_token * torch.ones_like(sample)).long()\n        \"\"\" Sample is the original sample\n        The mask indicates what tokens can be replaced (0 to not be replaced, 1 to be replaced)\n        Replace_with is a list of of the pad_token tokens\n        Here, (1-mask) creates the complement mask. (now, 0 indicates drop, 1 indicates to not drop)\n        1-1 = 0, 1-0 = 0\n        Multiplying by sample, retains the original tokens that are not to be kept, and applies the mask on the sample\n        Here, mask * replace_with does elementwise multiplication and adds the corresponding pad_token to the tokens replaced\n        \"\"\"\n        sample_out = (1 - mask) * sample + mask * replace_with\n        \n        return sample_out","metadata":{"execution":{"iopub.status.busy":"2024-07-02T01:19:39.998257Z","iopub.execute_input":"2024-07-02T01:19:39.998692Z","iopub.status.idle":"2024-07-02T01:19:40.006843Z","shell.execute_reply.started":"2024-07-02T01:19:39.998646Z","shell.execute_reply":"2024-07-02T01:19:40.005939Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"class SinusoidalPosEmb(nn.Module):\n    def __init__(self, dim):\n        super().__init__()\n        self.dim = dim\n\n    def forward(self, x):\n        # Get the device of the input tensor\n        device = x.device\n        \n        # Calculate half of the hidden size\n        half_dim = self.dim // 2\n        \n        emb = math.log(10000) / (half_dim - 1)\n        emb = torch.exp(torch.arange(half_dim, device=device) * -emb)\n        emb = x[:, None] * emb[None, :]\n        emb = torch.cat((emb.sin(), emb.cos()), dim=-1)\n        \n        return emb","metadata":{"execution":{"iopub.status.busy":"2024-07-02T01:19:40.674010Z","iopub.execute_input":"2024-07-02T01:19:40.674633Z","iopub.status.idle":"2024-07-02T01:19:40.681280Z","shell.execute_reply.started":"2024-07-02T01:19:40.674602Z","shell.execute_reply":"2024-07-02T01:19:40.680379Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"class NanoTransformer(nn.Module):\n    \"\"\"\n        This class implements a simplified Transformer model for sequence classification. \n        It uses an embedding layer for tokens, sinusoidal positional embeddings, \n        a Transformer, and a Linear layer.\n        \n        num_emb: The number of unique tokens in the vocabulary. (vocab_size)\n        output_size: The size of the output layer (number of classes). (4)\n        hidden_size: The dimension of the hidden layer in the Transformer block (default: 128)\n        num_heads: The number of heads in the multi-head attention layer (default: 4).\n    \"\"\"\n    def __init__(self, num_emb, output_size, hidden_size=128, num_heads=4):\n        \n        # Inherits from nn.Module's attributes\n        super(NanoTransformer, self).__init__()\n\n        # Create an embedding for each token\n        self.embedding = nn.Embedding(num_emb, hidden_size) # (vocab_size, 128)\n        \n        # Scaling down the embedding weights\n        self.embedding.weight.data = 0.001 * self.embedding.weight.data\n        \n        # Positional embedding\n        self.pos_emb = SinusoidalPosEmb(hidden_size)\n\n        # Multi-head attention\n        self.multihead_attn = nn.MultiheadAttention(hidden_size, num_heads = num_heads, batch_first = True)\n\n        # Linear layer\n        self.mlp = nn.Sequential(nn.Linear(hidden_size, hidden_size), # (batch_size, 128, 128)\n                                 nn.LayerNorm(hidden_size), # (batch_size, 128, 128)\n                                 nn.ELU(), # (batch_size, 128, 128)\n                                 nn.Linear(hidden_size, hidden_size)) # (batch_size, 128, 128)\n        \n        self.fc_out = nn.Linear(hidden_size, output_size) # (batch_size, 128, 128)\n\n    def forward(self, input_seq):\n        # batch_size, time_steps\n        batch_size, l = input_seq.shape # (32, 160)\n\n        input_embs = self.embedding(input_seq) # (32, 160) -> (32, 160, 128)\n        \n        # Add a unique embedding to each token embedding depending on it's position in the sequence\n        seq_indx = torch.arange(l) # (160)\n        \n        pos_emb = self.pos_emb(seq_indx).reshape(1, l, -1).expand(batch_size, l, -1) # (1, 160, 128) -> (32, 160, 128)\n        \n        pos_emb = pos_emb.to(device)\n        \n        embs = input_embs + pos_emb # (32, 160, 128) + (32, 160, 128)\n        \n        \n        output, attn_map = self.multihead_attn(embs, embs, embs) # (32, 160, 128)\n        \n        output = self.mlp(output) # (32, 160, 128)\n\n        return self.fc_out(output), attn_map # (32, 160, 4)","metadata":{"execution":{"iopub.status.busy":"2024-07-02T01:19:41.341293Z","iopub.execute_input":"2024-07-02T01:19:41.341979Z","iopub.status.idle":"2024-07-02T01:19:41.353472Z","shell.execute_reply.started":"2024-07-02T01:19:41.341948Z","shell.execute_reply":"2024-07-02T01:19:41.352523Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"# Set the device to GPU if available, otherwise use CPU\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\nlearning_rate = 1e-4\n\nnepochs = 20\n\nhidden_size = 256\n\noutput_size = 2\n\nnum_heads = 4\n\ntf_classifier = NanoTransformer(num_emb=len(vocab), output_size=2, hidden_size=hidden_size, num_heads=num_heads).to(device)\n\n# Initialize the optimizer\noptimizer = optim.Adam(tf_classifier.parameters(), lr=learning_rate, weight_decay=1e-4)\n\n# Cosine annealing scheduler to decay the learning rate\nlr_scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=nepochs, eta_min=0)\n\nloss_fn = nn.CrossEntropyLoss()\ntd = TokenDrop(prob=0.4)\n\ntraining_loss_list = []\ntest_loss_list = []\ntraining_acc_list = []\ntest_acc_list = []","metadata":{"execution":{"iopub.status.busy":"2024-07-02T01:19:42.264267Z","iopub.execute_input":"2024-07-02T01:19:42.265128Z","iopub.status.idle":"2024-07-02T01:19:42.343266Z","shell.execute_reply.started":"2024-07-02T01:19:42.265085Z","shell.execute_reply":"2024-07-02T01:19:42.342264Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"code","source":"# Number of model parameters\nnum_model_params = 0\nfor param in tf_classifier.parameters():\n    num_model_params += param.flatten().shape[0]\n\nprint(\"-This Model Has %d (Approximately %d Million) Parameters!\" % (num_model_params, num_model_params//1e6))","metadata":{"execution":{"iopub.status.busy":"2024-07-02T01:19:43.340102Z","iopub.execute_input":"2024-07-02T01:19:43.341003Z","iopub.status.idle":"2024-07-02T01:19:43.347220Z","shell.execute_reply.started":"2024-07-02T01:19:43.340968Z","shell.execute_reply":"2024-07-02T01:19:43.346258Z"},"trusted":true},"execution_count":31,"outputs":[{"name":"stdout","text":"-This Model Has 5516546 (Approximately 5 Million) Parameters!\n","output_type":"stream"}]},{"cell_type":"code","source":"# Initialize training and testing accuracy lists\ntrain_acc = 0\ntest_acc = 0\n\n# Loop over each epoch\nfor epoch in range(nepochs):\n    \n    tf_classifier.train()\n    \n    print(\"training:\")\n    \n    train_acc_count = 0\n    test_acc_count = 0\n    train_steps = 0\n\n    # Loop over each batch in the training dataset\n    for labels, texts in train_loader:\n        batch_size = labels.shape[0]\n        \n        # Transform the text to tokens and move to the GPU\n        text_tokens = text_transform(list(texts)).to(device)\n        labels = labels.to(device)\n        \n        # TokenDrop\n        text_tokens = td(text_tokens).to(device)\n        \n        # Get the model predictions\n        pred, _ = tf_classifier(text_tokens)\n        \n        # Compute the loss using cross-entropy loss\n        loss = loss_fn(pred[:, 0, :], labels)\n        \n        # Backpropagation and optimization step\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        \n        # Log the training loss\n        training_loss_list.append(loss.item())\n        \n        # Update training accuracy\n        train_acc_count += (pred[:, 0, :].argmax(1) == labels).sum().item()\n        train_steps += batch_size\n    \n    # Calculate average training accuracy\n    train_acc = (train_acc_count / train_steps)\n    training_acc_list.append(train_acc)\n    \n    # Update learning rate\n    lr_scheduler.step()\n    \n    # Set the model to evaluation mode\n    tf_classifier.eval()\n    \n    print(\"evaluating:\")\n    train_acc_count = 0\n    test_acc_count = 0\n    test_steps = 0\n    \n    # Loop over each batch in the testing dataset\n    with torch.no_grad():\n        \n        for labels, texts in test_loader:\n            batch_size = labels.shape[0]\n            \n            # Transform the text to tokens and move to the GPU\n            text_tokens = text_transform(list(texts)).to(device)\n            labels = labels.to(device)\n            \n            # Get the model predictions\n            pred, _ = tf_classifier(text_tokens)\n            \n            # Compute the loss using cross-entropy loss\n            loss = loss_fn(pred[:, 0, :], labels)\n            test_loss_list.append(loss.item())\n            \n            # Update testing accuracy\n            test_acc_count += (pred[:, 0, :].argmax(1) == labels).sum().item()\n            test_steps += batch_size\n        \n        # Calculate average testing accuracy\n        test_acc = (test_acc_count / test_steps)\n        test_acc_list.append(test_acc)\n\n    # Print out the results for this epoch\n    print(f'Epoch {epoch+1}/{nepochs}')\n    print(f'Training Accuracy: {train_acc*100:.2f}%')\n    print(f'Testing Accuracy: {test_acc*100:.2f}%')","metadata":{"execution":{"iopub.status.busy":"2024-07-02T01:20:18.390327Z","iopub.execute_input":"2024-07-02T01:20:18.391167Z","iopub.status.idle":"2024-07-02T01:24:52.469453Z","shell.execute_reply.started":"2024-07-02T01:20:18.391137Z","shell.execute_reply":"2024-07-02T01:24:52.467999Z"},"trusted":true},"execution_count":34,"outputs":[{"name":"stdout","text":"training\nevaluating\nEpoch 1/20\nTraining Accuracy: 49.93%\nTesting Accuracy: 49.76%\ntraining\nevaluating\nEpoch 2/20\nTraining Accuracy: 50.65%\nTesting Accuracy: 55.03%\ntraining\nevaluating\nEpoch 3/20\nTraining Accuracy: 58.28%\nTesting Accuracy: 72.46%\ntraining\nevaluating\nEpoch 4/20\nTraining Accuracy: 77.78%\nTesting Accuracy: 84.70%\ntraining\nevaluating\nEpoch 5/20\nTraining Accuracy: 82.53%\nTesting Accuracy: 84.07%\ntraining\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[34], line 21\u001b[0m\n\u001b[1;32m     18\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m labels\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# Transform the text to tokens and move to the GPU\u001b[39;00m\n\u001b[0;32m---> 21\u001b[0m text_tokens \u001b[38;5;241m=\u001b[39m \u001b[43mtext_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     22\u001b[0m labels \u001b[38;5;241m=\u001b[39m labels\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# TokenDrop\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torchtext/transforms.py:1027\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m   1022\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1023\u001b[0m \u001b[38;5;124;03m:param input: Input sequence or batch. The input type must be supported by the first transform in the sequence.\u001b[39;00m\n\u001b[1;32m   1024\u001b[0m \u001b[38;5;124;03m:type input: `Any`\u001b[39;00m\n\u001b[1;32m   1025\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1026\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m-> 1027\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1028\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torchtext/transforms.py:208\u001b[0m, in \u001b[0;36mTruncate.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    201\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    202\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    203\u001b[0m \u001b[38;5;124;03m    :param input: Input sequence or batch of sequence to be truncated\u001b[39;00m\n\u001b[1;32m    204\u001b[0m \u001b[38;5;124;03m    :type input: Union[List[Union[str, int]], List[List[Union[str, int]]]]\u001b[39;00m\n\u001b[1;32m    205\u001b[0m \u001b[38;5;124;03m    :return: Truncated sequence\u001b[39;00m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;124;03m    :rtype: Union[List[Union[str, int]], List[List[Union[str, int]]]]\u001b[39;00m\n\u001b[1;32m    207\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 208\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtruncate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_seq_len\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torchtext/functional.py:55\u001b[0m, in \u001b[0;36mtruncate\u001b[0;34m(input, max_seq_len)\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mjit\u001b[38;5;241m.\u001b[39misinstance(\u001b[38;5;28minput\u001b[39m, List[\u001b[38;5;28mstr\u001b[39m]):\n\u001b[1;32m     54\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m[:max_seq_len]\n\u001b[0;32m---> 55\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjit\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43misinstance\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mList\u001b[49m\u001b[43m[\u001b[49m\u001b[43mList\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m     56\u001b[0m     output: List[List[\u001b[38;5;28mint\u001b[39m]] \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     57\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m ids \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28minput\u001b[39m:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/jit/__init__.py:232\u001b[0m, in \u001b[0;36misinstance\u001b[0;34m(obj, target_type)\u001b[0m\n\u001b[1;32m    193\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21misinstance\u001b[39m(obj, target_type):\n\u001b[1;32m    194\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    195\u001b[0m \u001b[38;5;124;03m    This function provides for container type refinement in TorchScript. It can refine\u001b[39;00m\n\u001b[1;32m    196\u001b[0m \u001b[38;5;124;03m    parameterized containers of the List, Dict, Tuple, and Optional types. E.g. ``List[str]``,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    230\u001b[0m \u001b[38;5;124;03m        m(y)\u001b[39;00m\n\u001b[1;32m    231\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 232\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_isinstance\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_type\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/_jit_internal.py:1443\u001b[0m, in \u001b[0;36m_isinstance\u001b[0;34m(obj, target_type)\u001b[0m\n\u001b[1;32m   1441\u001b[0m origin_type \u001b[38;5;241m=\u001b[39m get_origin(target_type)\n\u001b[1;32m   1442\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m origin_type:\n\u001b[0;32m-> 1443\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcontainer_checker\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_type\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1445\u001b[0m \u001b[38;5;66;03m# Check to handle non-typed optional origin returns as none instead\u001b[39;00m\n\u001b[1;32m   1446\u001b[0m \u001b[38;5;66;03m#    of as optional in 3.7-3.8\u001b[39;00m\n\u001b[1;32m   1447\u001b[0m check_args_exist(target_type)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/_jit_internal.py:1378\u001b[0m, in \u001b[0;36mcontainer_checker\u001b[0;34m(obj, target_type)\u001b[0m\n\u001b[1;32m   1375\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m el \u001b[38;5;129;01min\u001b[39;00m obj:\n\u001b[1;32m   1376\u001b[0m     \u001b[38;5;66;03m# check if nested container, ex: List[List[str]]\u001b[39;00m\n\u001b[1;32m   1377\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m arg_origin:  \u001b[38;5;66;03m# processes nested container, ex: List[List[str]]\u001b[39;00m\n\u001b[0;32m-> 1378\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[43mcontainer_checker\u001b[49m\u001b[43m(\u001b[49m\u001b[43mel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43marg_type\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m   1379\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m   1380\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(el, arg_type):\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/_jit_internal.py:1380\u001b[0m, in \u001b[0;36mcontainer_checker\u001b[0;34m(obj, target_type)\u001b[0m\n\u001b[1;32m   1378\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m container_checker(el, arg_type):\n\u001b[1;32m   1379\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m-> 1380\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43marg_type\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m   1381\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m   1382\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}]},{"cell_type":"code","source":"# Save only the model weights\ntorch.save(tf_classifier.state_dict(), 'transformer_sentiment_analysis.pth')","metadata":{"execution":{"iopub.status.busy":"2024-07-02T01:35:39.793231Z","iopub.execute_input":"2024-07-02T01:35:39.793607Z","iopub.status.idle":"2024-07-02T01:35:39.858827Z","shell.execute_reply.started":"2024-07-02T01:35:39.793577Z","shell.execute_reply":"2024-07-02T01:35:39.857869Z"},"trusted":true},"execution_count":67,"outputs":[]},{"cell_type":"code","source":"def predict_single_text(model, text):\n    tokens = text_transform([text]).to(device)\n    with torch.no_grad():\n        logits, att_map = model(tokens)\n    probs = F.softmax(logits[:, 0, :], dim=-1)\n    label = torch.argmax(probs, dim  = 1)\n    print(probs)\n    \n    att_map = att_map.squeeze(0)\n    cls_att_weights = att_map[0]\n    # Get top 10 tokens with the highest attention weights\n    top20 = cls_att_weights.argsort(descending=True)[:20]\n    top20_tokens = [vocab.lookup_token(tokens[0][idx].item()) for idx in top20]\n    \n    print(\"Top 10 tokens with highest attention:\", top20_tokens)\n    \n    if label.item() == 0:\n        return 'negative'\n    else:\n        return 'positive'\n\n    \n# Example usage:\ntext = \"Meryl Streep as Kate, a woman dying of cancer, performs her role admirably. No wonder she was up for an Oscar. In the part she proves that caring and nurturing housewives are just as important as their sisters out in the business world. And the lesson she teaches about life's expectations and their lack of fulfillment as the relationship grows, that is the most important thing she teaches her daughter. We can expect too much of our mates. Realize that there are many slips and forgiveness or understanding are the main ingredients of a happy life. This is a sombre movie and the ending though sad, shows reconciliation between the father and daughter. I give this one a ten.\"\noutput = predict_single_text(tf_classifier, text)\n\nprint(\"Predicted probabilities:\", output)","metadata":{"execution":{"iopub.status.busy":"2024-07-02T01:34:41.668277Z","iopub.execute_input":"2024-07-02T01:34:41.668861Z","iopub.status.idle":"2024-07-02T01:34:41.788518Z","shell.execute_reply.started":"2024-07-02T01:34:41.668825Z","shell.execute_reply":"2024-07-02T01:34:41.787607Z"},"trusted":true},"execution_count":64,"outputs":[{"name":"stdout","text":"tensor([[0.0939, 0.9061]], device='cuda:0')\nTop 10 tokens with highest attention: ['re', 'ep', '<unk>', '▁', 't', 'l', 'ery', '▁lack', '▁daughter', '▁teaches', '▁part', '▁sad', '▁daughter', '▁as', 'his', '▁', '<unk>', '▁world', '▁happy', '▁sisters']\nPredicted probabilities: positive\n","output_type":"stream"}]}]}