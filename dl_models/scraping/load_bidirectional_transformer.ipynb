{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1bb50312-5779-4584-ab5f-e1c296bdcb05",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kennykguo/anaconda3/envs/deep-learning/lib/python3.8/site-packages/torchtext/data/__init__.py:4: UserWarning: \n",
      "/!\\ IMPORTANT WARNING ABOUT TORCHTEXT STATUS /!\\ \n",
      "Torchtext is deprecated and the last released version will be 0.18 (this one). You can silence this warning by calling the following at the beginnign of your scripts: `import torchtext; torchtext.disable_torchtext_deprecation_warning()`\n",
      "  warnings.warn(torchtext._TORCHTEXT_DEPRECATION_MSG)\n",
      "/home/kennykguo/anaconda3/envs/deep-learning/lib/python3.8/site-packages/torchtext/vocab/__init__.py:4: UserWarning: \n",
      "/!\\ IMPORTANT WARNING ABOUT TORCHTEXT STATUS /!\\ \n",
      "Torchtext is deprecated and the last released version will be 0.18 (this one). You can silence this warning by calling the following at the beginnign of your scripts: `import torchtext; torchtext.disable_torchtext_deprecation_warning()`\n",
      "  warnings.warn(torchtext._TORCHTEXT_DEPRECATION_MSG)\n",
      "/home/kennykguo/anaconda3/envs/deep-learning/lib/python3.8/site-packages/torchtext/utils.py:4: UserWarning: \n",
      "/!\\ IMPORTANT WARNING ABOUT TORCHTEXT STATUS /!\\ \n",
      "Torchtext is deprecated and the last released version will be 0.18 (this one). You can silence this warning by calling the following at the beginnign of your scripts: `import torchtext; torchtext.disable_torchtext_deprecation_warning()`\n",
      "  warnings.warn(torchtext._TORCHTEXT_DEPRECATION_MSG)\n",
      "/home/kennykguo/anaconda3/envs/deep-learning/lib/python3.8/site-packages/torchtext/transforms.py:4: UserWarning: \n",
      "/!\\ IMPORTANT WARNING ABOUT TORCHTEXT STATUS /!\\ \n",
      "Torchtext is deprecated and the last released version will be 0.18 (this one). You can silence this warning by calling the following at the beginnign of your scripts: `import torchtext; torchtext.disable_torchtext_deprecation_warning()`\n",
      "  warnings.warn(torchtext._TORCHTEXT_DEPRECATION_MSG)\n",
      "/home/kennykguo/anaconda3/envs/deep-learning/lib/python3.8/site-packages/torchtext/functional.py:4: UserWarning: \n",
      "/!\\ IMPORTANT WARNING ABOUT TORCHTEXT STATUS /!\\ \n",
      "Torchtext is deprecated and the last released version will be 0.18 (this one). You can silence this warning by calling the following at the beginnign of your scripts: `import torchtext; torchtext.disable_torchtext_deprecation_warning()`\n",
      "  warnings.warn(torchtext._TORCHTEXT_DEPRECATION_MSG)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchtext\n",
    "from torchtext.data.functional import generate_sp_model, load_sp_model, sentencepiece_tokenizer, sentencepiece_numericalizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "import torchtext.transforms as T\n",
    "from torch.nn import functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import io\n",
    "import math\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.utils.data.dataloader as dataloader\n",
    "import os\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "11b534d2-1e2f-4653-b032-2ecad68df2c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the tokenizer and vocabulary\n",
    "model_path = \"models/gpt.pth\"\n",
    "tokenizer_path = \"sentencepiece/transformer.model\"\n",
    "vocab_path = \"sentencepiece/transformer.vocab\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "81bf08bd-93ad-4e25-8da7-099dd597eaf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def yield_tokens(file_path):\n",
    "    with io.open(file_path, encoding='utf-8') as f:\n",
    "        # Iterate through each line in the file\n",
    "        for line in f:\n",
    "            # Accesses the vocab file, splits the line by tab, and gets the first entry (the actual token)\n",
    "            # Yield the token from the first column (split by tab)\n",
    "            yield [line.split(\"\\t\")[0]]\n",
    "\n",
    "# Build a vocabulary from the tokens yielded by the yield_tokens function\n",
    "    # <pad> is a padding token that is added to the end of a sentence to ensure the length of all sequences in a batch is the same\n",
    "    # <sos> signals the \"Start-Of-Sentence\" aka the start of the sequence\n",
    "    # <eos> signal the \"End-Of-Sentence\" aka the end of the sequence\n",
    "    # <unk> \"unknown\" token is used if a token is not contained in the vocab\n",
    "# From torchtext library (build_vocab_from_iterator)\n",
    "# Builds a generator object, that is treated like an iterator\n",
    "vocab = build_vocab_from_iterator(yield_tokens(\"../SentencePiece/transformer.vocab\"), specials=['<cls>', '<pad>', '<eos>', '<unk>'], special_first=True)\n",
    "\n",
    "# Set the default index for unknown tokens to the index of the '<unk>' token\n",
    "vocab.set_default_index(vocab['<unk>'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3046995f-0383-4618-84fe-4f95f8f8602d",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_transform = T.Sequential(\n",
    "    T.SentencePieceTokenizer(tokenizer_path),\n",
    "    T.VocabTransform(vocab),\n",
    "    T.AddToken(vocab['<cls>'], begin=True),\n",
    "    T.Truncate(max_seq_len=254),\n",
    "    T.AddToken(vocab['<eos>'], begin=False),\n",
    "    T.ToTensor(padding_value=vocab['<pad>']),\n",
    "    T.PadTransform(max_length=256, pad_value=0),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6ff31cc4-89d8-4022-af0f-4fe299bc35bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# No changes\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, n_embd):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4 * n_embd),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * n_embd, n_embd),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "# Updated to unpack the tuple\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, n_embd, n_head):\n",
    "        super().__init__()\n",
    "        head_size = n_embd // n_head\n",
    "        self.sa = MultiHeadAttention(n_head, head_size)\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.ffwd = FeedForward(n_embd)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "\n",
    "    def forward(self, x):\n",
    "        sa_out, attn_weights = self.sa(self.ln1(x))\n",
    "        x = x + sa_out  # Residual connection\n",
    "        x = x + self.ffwd(self.ln2(x))\n",
    "        return x, attn_weights\n",
    "\n",
    "\n",
    "# Updated to return attention weights\n",
    "\n",
    "class Head(nn.Module):\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape\n",
    "        k = self.key(x)\n",
    "        q = self.query(x)\n",
    "        wei = q @ k.transpose(-2, -1) * C ** -0.5\n",
    "        wei = F.softmax(wei, dim=-1)\n",
    "        wei = self.dropout(wei)\n",
    "        v = self.value(x)\n",
    "        out = wei @ v\n",
    "        return out, wei  # Return attention weights\n",
    "\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(n_embd, n_embd)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = []\n",
    "        attn_weights = []\n",
    "        for h in self.heads:\n",
    "            head_out, head_attn = h(x)\n",
    "            out.append(head_out)\n",
    "            attn_weights.append(head_attn)\n",
    "        out = torch.cat(out, dim=-1)\n",
    "        out = self.dropout(self.proj(out))\n",
    "        attn_weights = torch.stack(attn_weights, dim=1)  # (B, num_heads, T, T)\n",
    "        return out, attn_weights\n",
    "\n",
    "\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "        self.position_embedding_table = nn.Embedding(block_size + 1, n_embd)\n",
    "        self.blocks = nn.ModuleList([Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
    "        self.ln_f = nn.LayerNorm(n_embd)\n",
    "        self.lm_head = nn.Linear(n_embd, output_size)\n",
    " \n",
    "    def forward(self, idx):\n",
    "        B, T = idx.shape\n",
    "        tok_emb = self.token_embedding_table(idx)  # (B, T, C)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=idx.device))  # (T, C)\n",
    "        x = tok_emb + pos_emb  # (B, T, C) - broadcasting\n",
    "\n",
    "        for i, block in enumerate(self.blocks):\n",
    "            x, attn_weights = block(x)  # Save attention weights from the last block\n",
    "\n",
    "        x = self.ln_f(x)\n",
    "        logits = self.lm_head(x[:, 0, :])  # (B, output_size) - we use the CLS token representation\n",
    "\n",
    "        return logits, attn_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ef11f7e5-8e4e-467c-a1e4-7786f84c070b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "block_size = 256\n",
    "n_embd = 768\n",
    "n_head = 12\n",
    "n_layer = 2\n",
    "dropout = 0.4\n",
    "output_size = 2\n",
    "vocab_size = len(vocab)\n",
    "learning_rate = 5e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2a4861b9-0a02-4c47-aaca-e276783f7ff8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "model = Transformer()\n",
    "model.load_state_dict(torch.load(model_path))\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "print(\"Model loaded successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a64a0665-0420-4297-9840-666db8340ffd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 256])\n",
      "tensor([[0.1060, 0.8940]], device='cuda:0')\n",
      "Predicted probabilities: positive\n"
     ]
    }
   ],
   "source": [
    "def predict_single_text(model, text):\n",
    "    tokens = text_transform([text]).to(device)\n",
    "    # tokens = tokens.unsqueeze(0)  # Add batch dimension\n",
    "    print(tokens.shape)\n",
    "    with torch.no_grad():\n",
    "        logits, _ = model(tokens)\n",
    "    probs = F.softmax(logits, dim=-1)\n",
    "    label = torch.argmax(probs, dim  = 1)\n",
    "    print(probs)\n",
    "    if label.item() == 0:\n",
    "        return 'negative'\n",
    "    else:\n",
    "        return 'positive'\n",
    "\n",
    "# Example usage:\n",
    "text = \"Meryl Streep as Kate, a woman dying of cancer, performs her role admirably. No wonder she was up for an Oscar. In the part she proves that caring and nurturing housewives are just as important as their sisters out in the business world. And the lesson she teaches about life's expectations and their lack of fulfillment as the relationship grows, that is the most important thing she teaches her daughter. We can expect too much of our mates. Realize that there are many slips and forgiveness or understanding are the main ingredients of a happy life. This is a sombre movie and the ending though sad, shows reconciliation between the father and daughter. I give this one a ten.\"\n",
    "\n",
    "\n",
    "output = predict_single_text(model, text)\n",
    "\n",
    "\n",
    "print(\"Predicted probabilities:\", output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cde5f6d4-f68b-453c-986d-343919faab11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 256])\n",
      "tensor([[0.1060, 0.8940]], device='cuda:0')\n",
      "torch.Size([1, 12, 256, 256])\n",
      "Top 10 tokens with highest attention: ['▁the', '.', '.', '▁the', '.', '▁the', '.', '▁the', '▁the', '▁the']\n",
      "Predicted sentiment: positive\n"
     ]
    }
   ],
   "source": [
    "def predict_single_text(model, text, text_transform, vocab):\n",
    "    # Transform the input text to tokens\n",
    "    tokens = text_transform([text]).to(device)\n",
    "    print(tokens.shape)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        logits, attn_weights = model(tokens)\n",
    "    \n",
    "    # Get the probabilities and predicted label\n",
    "    probs = F.softmax(logits, dim=-1)\n",
    "    label = torch.argmax(probs, dim=1)\n",
    "    \n",
    "    print(probs)\n",
    "    \n",
    "    print(attn_weights.shape)\n",
    "    \n",
    "    # Example of analyzing attention maps\n",
    "    # Get the attention map for the last head and the first sample\n",
    "    att_map = attn_weights[0, -1, :, :]  # Shape: (T, T)\n",
    "    \n",
    "    # Sum the attention weights across all tokens for the [CLS] token\n",
    "    att_weights = att_map[0]  # Attention weights for the [CLS] token\n",
    "    \n",
    "    # Get top 10 tokens with the highest attention weights\n",
    "    top10 = att_weights.argsort(descending=True)[:10]\n",
    "    top10_tokens = [vocab.lookup_token(tokens[0][idx].item()) for idx in top10]\n",
    "    \n",
    "    print(\"Top 10 tokens with highest attention:\", top10_tokens)\n",
    "    \n",
    "    if label.item() == 0:\n",
    "        return 'negative'\n",
    "    else:\n",
    "        return 'positive'\n",
    "\n",
    "# Example usage:\n",
    "text = \"Meryl Streep as Kate, a woman dying of cancer, performs her role admirably. No wonder she was up for an Oscar. In the part she proves that caring and nurturing housewives are just as important as their sisters out in the business world. And the lesson she teaches about life's expectations and their lack of fulfillment as the relationship grows, that is the most important thing she teaches her daughter. We can expect too much of our mates. Realize that there are many slips and forgiveness or understanding are the main ingredients of a happy life. This is a sombre movie and the ending though sad, shows reconciliation between the father and daughter. I give this one a ten.\"\n",
    "\n",
    "\n",
    "output = predict_single_text(model, text, text_transform, vocab)\n",
    "print(\"Predicted sentiment:\", output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (deep-learning)",
   "language": "python",
   "name": "deep-learning"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
