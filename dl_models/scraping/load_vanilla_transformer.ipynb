{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8fafc6c2-d9d7-4ed4-bf0b-e7802510db8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.kaggle.com/code/emirkocak/in-depth-series-sentiment-analysis-w-transformers\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchtext\n",
    "from torchtext.data.functional import generate_sp_model, load_sp_model, sentencepiece_tokenizer, sentencepiece_numericalizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "import torchtext.transforms as T\n",
    "from torch.nn import functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import io\n",
    "import math\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.utils.data.dataloader as dataloader\n",
    "import os\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9b9f7afe-06eb-484c-895c-d3b29748efe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the tokenizer and vocabulary\n",
    "model_path = \"../models/transformer_sentiment_analysis.pth\"\n",
    "tokenizer_path = \"../SentencePiece/transformer.model\"\n",
    "vocab_path = \"../SentencePiece/transformer.vocab\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "76dadbc5-e94c-4876-b457-24f149ef6ad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def yield_tokens(file_path):\n",
    "    with io.open(file_path, encoding='utf-8') as f:\n",
    "        # Iterate through each line in the file\n",
    "        for line in f:\n",
    "            # Accesses the vocab file, splits the line by tab, and gets the first entry (the actual token)\n",
    "            # Yield the token from the first column (split by tab)\n",
    "            yield [line.split(\"\\t\")[0]]\n",
    "\n",
    "# Build a vocabulary from the tokens yielded by the yield_tokens function\n",
    "    # <pad> is a padding token that is added to the end of a sentence to ensure the length of all sequences in a batch is the same\n",
    "    # <sos> signals the \"Start-Of-Sentence\" aka the start of the sequence\n",
    "    # <eos> signal the \"End-Of-Sentence\" aka the end of the sequence\n",
    "    # <unk> \"unknown\" token is used if a token is not contained in the vocab\n",
    "# From torchtext library (build_vocab_from_iterator)\n",
    "# Builds a generator object, that is treated like an iterator\n",
    "vocab = build_vocab_from_iterator(yield_tokens(\"../SentencePiece/transformer.vocab\"), specials=['<cls>', '<pad>', '<eos>', '<unk>'], special_first=True)\n",
    "\n",
    "# Set the default index for unknown tokens to the index of the '<unk>' token\n",
    "vocab.set_default_index(vocab['<unk>'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c3a98bcd-6513-4e0e-912e-4d06f61c6ef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_transform = T.Sequential(\n",
    "    T.SentencePieceTokenizer(tokenizer_path),\n",
    "    T.VocabTransform(vocab),\n",
    "    T.AddToken(vocab['<cls>'], begin=True),\n",
    "    T.Truncate(max_seq_len=254),\n",
    "    T.AddToken(vocab['<eos>'], begin=False),\n",
    "    T.ToTensor(padding_value=vocab['<pad>']),\n",
    "    T.PadTransform(max_length=256, pad_value=0),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "16052c77-2ff4-4bc9-84a7-f550c2455a6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenDrop(nn.Module):\n",
    "    \"\"\" For a batch of tokens indices, randomly replace a non-specical token with <pad>.\n",
    "    prob (float): probability of dropping a token\n",
    "    pad_token (int): index for the <pad> token\n",
    "    num_special (int): Number of special tokens, assumed to be at the start of the vocab\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, prob=0.1, pad_token=0, num_special=4):\n",
    "        self.prob = prob\n",
    "        self.num_special = num_special\n",
    "        self.pad_token = pad_token\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        # Randomly sample a bernoulli distribution with p = prob\n",
    "        # Create a mask where 1 means we will replace that token\n",
    "        # Discrete probability distribution\n",
    "        # Here we want to treat the ones as the indexes to drop\n",
    "        mask = torch.bernoulli(self.prob * torch.ones_like(sample)).long()\n",
    "        \n",
    "        # Only replace if the token is not a special token\n",
    "        # Ones or zeros. If cannot drop, 0, if can drop, 1\n",
    "        can_drop = (sample >= self.num_special).long()\n",
    "        # Multiply together to get the corresponding tokens to be dropped and not dropped\n",
    "        # Here, 1 represents drop, 0 represents do not drop\n",
    "        mask = mask * can_drop\n",
    "\n",
    "        # Make a mask of pad_token to use for replacing dropped indices with the pad_token\n",
    "        replace_with = (self.pad_token * torch.ones_like(sample)).long()\n",
    "        \"\"\" Sample is the original sample\n",
    "        The mask indicates what tokens can be replaced (0 to not be replaced, 1 to be replaced)\n",
    "        Replace_with is a list of of the pad_token tokens\n",
    "        Here, (1-mask) creates the complement mask. (now, 0 indicates drop, 1 indicates to not drop)\n",
    "        1-1 = 0, 1-0 = 0\n",
    "        Multiplying by sample, retains the original tokens that are not to be kept, and applies the mask on the sample\n",
    "        Here, mask * replace_with does elementwise multiplication and adds the corresponding pad_token to the tokens replaced\n",
    "        \"\"\"\n",
    "        sample_out = (1 - mask) * sample + mask * replace_with\n",
    "        \n",
    "        return sample_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "63358cd9-d18d-4569-9512-e4ea88cb5626",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SinusoidalPosEmb(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Get the device of the input tensor\n",
    "        device = x.device\n",
    "        \n",
    "        # Calculate half of the hidden size\n",
    "        half_dim = self.dim // 2\n",
    "        \n",
    "        emb = math.log(10000) / (half_dim - 1)\n",
    "        emb = torch.exp(torch.arange(half_dim, device=device) * -emb)\n",
    "        emb = x[:, None] * emb[None, :]\n",
    "        emb = torch.cat((emb.sin(), emb.cos()), dim=-1)\n",
    "        \n",
    "        return emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "defcd2a8-0bc5-41e1-aaf1-c7749d5f663d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NanoTransformer(nn.Module):\n",
    "    \"\"\"\n",
    "        This class implements a simplified Transformer model for sequence classification. \n",
    "        It uses an embedding layer for tokens, sinusoidal positional embeddings, \n",
    "        a Transformer, and a Linear layer.\n",
    "        \n",
    "        num_emb: The number of unique tokens in the vocabulary. (vocab_size)\n",
    "        output_size: The size of the output layer (number of classes). (4)\n",
    "        hidden_size: The dimension of the hidden layer in the Transformer block (default: 128)\n",
    "        num_heads: The number of heads in the multi-head attention layer (default: 4).\n",
    "    \"\"\"\n",
    "    def __init__(self, num_emb, output_size, hidden_size=128, num_heads=4):\n",
    "        \n",
    "        # Inherits from nn.Module's attributes\n",
    "        super(NanoTransformer, self).__init__()\n",
    "\n",
    "        # Create an embedding for each token\n",
    "        self.embedding = nn.Embedding(num_emb, hidden_size) # (vocab_size, 128)\n",
    "        \n",
    "        # Scaling down the embedding weights\n",
    "        self.embedding.weight.data = 0.001 * self.embedding.weight.data\n",
    "        \n",
    "        # Positional embedding\n",
    "        self.pos_emb = SinusoidalPosEmb(hidden_size)\n",
    "\n",
    "        # Multi-head attention\n",
    "        self.multihead_attn = nn.MultiheadAttention(hidden_size, num_heads = num_heads, batch_first = True)\n",
    "\n",
    "        # Linear layer\n",
    "        self.mlp = nn.Sequential(nn.Linear(hidden_size, hidden_size), # (batch_size, 128, 128)\n",
    "                                 nn.LayerNorm(hidden_size), # (batch_size, 128, 128)\n",
    "                                 nn.ELU(), # (batch_size, 128, 128)\n",
    "                                 nn.Linear(hidden_size, hidden_size)) # (batch_size, 128, 128)\n",
    "        \n",
    "        self.fc_out = nn.Linear(hidden_size, output_size) # (batch_size, 128, 128)\n",
    "\n",
    "    def forward(self, input_seq):\n",
    "        # batch_size, time_steps\n",
    "        batch_size, l = input_seq.shape # (32, 160)\n",
    "\n",
    "        input_embs = self.embedding(input_seq) # (32, 160) -> (32, 160, 128)\n",
    "        \n",
    "        # Add a unique embedding to each token embedding depending on it's position in the sequence\n",
    "        seq_indx = torch.arange(l) # (160)\n",
    "        \n",
    "        pos_emb = self.pos_emb(seq_indx).reshape(1, l, -1).expand(batch_size, l, -1) # (1, 160, 128) -> (32, 160, 128)\n",
    "        \n",
    "        pos_emb = pos_emb.to(device)\n",
    "        \n",
    "        embs = input_embs + pos_emb # (32, 160, 128) + (32, 160, 128)\n",
    "        \n",
    "        \n",
    "        output, attn_map = self.multihead_attn(embs, embs, embs) # (32, 160, 128)\n",
    "        \n",
    "        output = self.mlp(output) # (32, 160, 128)\n",
    "\n",
    "        return self.fc_out(output), attn_map # (32, 160, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9e93c2df-2bb1-44b5-a3b4-51108441feec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the device to GPU if available, otherwise use CPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "learning_rate = 1e-4\n",
    "hidden_size = 256\n",
    "output_size = 2\n",
    "num_heads = 4\n",
    "\n",
    "tf_classifier = NanoTransformer(num_emb=len(vocab), output_size=2, hidden_size=hidden_size, num_heads=num_heads).to(device)\n",
    "tf_classifier = tf_classifier.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "825b26ef-58bd-4c11-9ef9-c1a3a7686074",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NanoTransformer(\n",
       "  (embedding): Embedding(20003, 256)\n",
       "  (pos_emb): SinusoidalPosEmb()\n",
       "  (multihead_attn): MultiheadAttention(\n",
       "    (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
       "  )\n",
       "  (mlp): Sequential(\n",
       "    (0): Linear(in_features=256, out_features=256, bias=True)\n",
       "    (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "    (2): ELU(alpha=1.0)\n",
       "    (3): Linear(in_features=256, out_features=256, bias=True)\n",
       "  )\n",
       "  (fc_out): Linear(in_features=256, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the model\n",
    "tf_classifier.load_state_dict(torch.load(model_path))\n",
    "tf_classifier.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "814d83b8-4484-4890-9d21-0f1a9419815b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 256, 256])\n",
      "Probabilities: tensor([[0.9966, 0.0034]], device='cuda:0')\n",
      "Sentence: Truly bad and easily the worst episode I have ever seen....ever. They tried to make up for it by giving it the, 'we know we are doing this' routine. That would have been funny if it weren't for the fact that 'The Simpsons' had already done it. And it still wouldn't make up for it if they had come up with the idea in the first place. The flashbacks took place as part of the usual character's (mainly J.D's) fantasies. The flashbacks weren't even of actual events that occurred, just compilations of say, J.D falling over or, i don't know.... Elliott falling over. If I wanted to watch a Scrubs compilation i'd go on youtube and not waste half an hour of my life. Scrubs has ultimately fallen into the trap that most sit-coms have to, and it disappoints me, they managed to go 5 and a quarter seasons without an episode like this.  I was hoping that scrubs wouldn't have to be that kind of sit-com. And just as a passing thought, why the hell was Dr.Cox bald? \n",
      " -> Predicted label: negative\n",
      "\n",
      "▁worst, ▁waste, ▁bad, ▁fantasies, ▁idea, ▁hour, ▁easily, ▁half, nd, ▁falling\n",
      "torch.Size([1, 256, 256])\n",
      "Probabilities: tensor([[0.1141, 0.8859]], device='cuda:0')\n",
      "Sentence: Meryl Streep as Kate, a woman dying of cancer, performs her role admirably. No wonder she was up for an Oscar. In the part she proves that caring and nurturing housewives are just as important as their sisters out in the business world. And the lesson she teaches about life's expectations and their lack of fulfillment as the relationship grows, that is the most important thing she teaches her daughter. We can expect too much of our mates. Realize that there are many slips and forgiveness or understanding are the main ingredients of a happy life. This is a sombre movie and the ending though sad, shows reconciliation between the father and daughter. I give this one a ten. \n",
      " -> Predicted label: positive\n",
      "\n",
      "ery, ate, ize, ▁between, ▁part, ▁though, ▁sad, ▁proves, ▁sisters, ▁daughter\n"
     ]
    }
   ],
   "source": [
    "# Example test cases\n",
    "test_sentences = [\n",
    "    \"Truly bad and easily the worst episode I have ever seen....ever. They tried to make up for it by giving it the, 'we know we are doing this' routine. That would have been funny if it weren't for the fact that 'The Simpsons' had already done it. And it still wouldn't make up for it if they had come up with the idea in the first place. The flashbacks took place as part of the usual character's (mainly J.D's) fantasies. The flashbacks weren't even of actual events that occurred, just compilations of say, J.D falling over or, i don't know.... Elliott falling over. If I wanted to watch a Scrubs compilation i'd go on youtube and not waste half an hour of my life. Scrubs has ultimately fallen into the trap that most sit-coms have to, and it disappoints me, they managed to go 5 and a quarter seasons without an episode like this.  I was hoping that scrubs wouldn't have to be that kind of sit-com. And just as a passing thought, why the hell was Dr.Cox bald?\",\n",
    "    \"Meryl Streep as Kate, a woman dying of cancer, performs her role admirably. No wonder she was up for an Oscar. In the part she proves that caring and nurturing housewives are just as important as their sisters out in the business world. And the lesson she teaches about life's expectations and their lack of fulfillment as the relationship grows, that is the most important thing she teaches her daughter. We can expect too much of our mates. Realize that there are many slips and forgiveness or understanding are the main ingredients of a happy life. This is a sombre movie and the ending though sad, shows reconciliation between the father and daughter. I give this one a ten.\"\n",
    "]\n",
    "\n",
    "# Transform test sentences into tokens\n",
    "test_tokens = [text_transform([sentence]).to(device) for sentence in test_sentences]\n",
    "\n",
    "# Perform inference\n",
    "with torch.no_grad():\n",
    "    # Loop over all tokenized paragraphs\n",
    "    for idx, tokens in enumerate(test_tokens):\n",
    "        logits, attn_map = tf_classifier(tokens)\n",
    "        print(attn_map.shape)\n",
    "        logits = logits[:, 0, :]\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "        print(\"Probabilities:\", probs)\n",
    "        label = torch.argmax(probs, dim=1)\n",
    "        if label.item() == 0:\n",
    "            label = 'negative'\n",
    "        else:\n",
    "            label = 'positive'\n",
    "        \n",
    "        print(f\"Sentence: {test_sentences[idx]} \\n -> Predicted label: {label}\\n\")\n",
    "        att_map = attn_map[0, 0] # (1, 256) -> Tokens for the class\n",
    "        top10 = att_map.argsort(descending=True)[:10]\n",
    "        top10_tokens = vocab.lookup_tokens(tokens[0, top10].cpu().numpy())\n",
    "        print(\", \".join(top10_tokens))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (deep-learning)",
   "language": "python",
   "name": "deep-learning"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
